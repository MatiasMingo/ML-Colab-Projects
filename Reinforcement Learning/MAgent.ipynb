{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MAgent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNAvsOJ4E+lAq6GwWzcqqLI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasMingo/ML-Colab-Projects/blob/main/MAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgL2bwz6s3rp"
      },
      "source": [
        "# MAgent multi-agent system framework implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9KNLMS0snBZ"
      },
      "source": [
        "!pip install magent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "IWxxtocbxD6q"
      },
      "source": [
        "#@title BaseModel class\n",
        "\n",
        "   \n",
        "\"\"\" base model classes\"\"\"\n",
        "\n",
        "try:\n",
        "    import thread\n",
        "except ImportError:\n",
        "    import _thread as thread\n",
        "\n",
        "import multiprocessing\n",
        "import multiprocessing.connection\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class BaseModel:\n",
        "    def __init__(self, env, handle, *args, **kwargs):\n",
        "        \"\"\" init\n",
        "        Parameters\n",
        "        ----------\n",
        "        env: Environment\n",
        "            env\n",
        "        handle: GroupHandle\n",
        "            handle of this group, handles are returned by env.get_handles()\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def infer_action(self, raw_obs, ids, *args, **kwargs):\n",
        "        \"\"\" infer action for a group of agents\n",
        "        Parameters\n",
        "        ----------\n",
        "        raw_obs: tuple\n",
        "            raw_obs is a tuple of (view, feature)\n",
        "            view is a numpy array, its shape is n * view_width * view_height * n_channel\n",
        "                                   it contains the spatial local observation for all the agents\n",
        "            feature is a numpy array, its shape is n * feature_size\n",
        "                                   it contains the non-spatial feature for all the agents\n",
        "        ids: numpy array of int32\n",
        "            the unique id of every agents\n",
        "        args:\n",
        "            additional custom args\n",
        "        kwargs:\n",
        "            additional custom args\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def train(self, sample_buffer, **kwargs):\n",
        "        \"\"\" feed new samples and train\n",
        "        Parameters\n",
        "        ----------\n",
        "        sample_buffer: EpisodesBuffer\n",
        "            a buffer contains transitions of agents\n",
        "        Returns\n",
        "        -------\n",
        "        loss and estimated mean state value\n",
        "        \"\"\"\n",
        "        return 0, 0    # loss, mean value\n",
        "\n",
        "    def save(self, *args, **kwargs):\n",
        "        \"\"\" save the model \"\"\"\n",
        "        pass\n",
        "\n",
        "    def load(self, *args, **kwargs):\n",
        "        \"\"\" load the model \"\"\"\n",
        "        pass"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "bPQDptBGwzHN"
      },
      "source": [
        "#@title TensorFlow base model\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class TFBaseModel(BaseModel):\n",
        "    \"\"\"base model for tensorflow model\"\"\"\n",
        "    def __init__(self, env, handle, name, subclass_name):\n",
        "        \"\"\"init a model\n",
        "        Parameters\n",
        "        ----------\n",
        "        env: magent.Environment\n",
        "        handle: handle (ctypes.c_int32)\n",
        "        name: str\n",
        "        subclass_name: str\n",
        "            name of subclass\n",
        "        \"\"\"\n",
        "        BaseModel.__init__(self, env, handle)\n",
        "        self.name = name\n",
        "        self.subclass_name = subclass_name\n",
        "\n",
        "    def save(self, dir_name, epoch):\n",
        "        \"\"\"save model to dir\n",
        "        Parameters\n",
        "        ----------\n",
        "        dir_name: str\n",
        "            name of the directory\n",
        "        epoch: int\n",
        "        \"\"\"\n",
        "        if not os.path.exists(dir_name):\n",
        "            os.mkdir(dir_name)\n",
        "        dir_name = os.path.join(dir_name, self.name)\n",
        "        if not os.path.exists(dir_name):\n",
        "            os.mkdir(dir_name)\n",
        "        model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.name)\n",
        "        saver = tf.train.Saver(model_vars)\n",
        "        saver.save(self.sess, os.path.join(dir_name, (self.subclass_name + \"_%d\") % epoch))\n",
        "\n",
        "    def load(self, dir_name, epoch=0, name=None):\n",
        "        \"\"\"save model to dir\n",
        "        Parameters\n",
        "        ----------\n",
        "        dir_name: str\n",
        "            name of the directory\n",
        "        epoch: int\n",
        "        \"\"\"\n",
        "        if name is None or name == self.name:  # the name of saved model is the same as ours\n",
        "            dir_name = os.path.join(dir_name, self.name)\n",
        "            model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.name)\n",
        "            saver = tf.train.Saver(model_vars)\n",
        "            saver.restore(self.sess, os.path.join(dir_name, (self.subclass_name + \"_%d\") % epoch))\n",
        "        else:  # load a checkpoint with different name\n",
        "            backup_graph = tf.get_default_graph()\n",
        "            kv_dict = {}\n",
        "\n",
        "            # load checkpoint from another saved graph\n",
        "            with tf.Graph().as_default(), tf.Session() as sess:\n",
        "                tf.train.import_meta_graph(os.path.join(dir_name, name, (self.subclass_name + \"_%d\") % epoch + \".meta\"))\n",
        "                dir_name = os.path.join(dir_name, name)\n",
        "                model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, name)\n",
        "                sess.run(tf.global_variables_initializer())\n",
        "                saver = tf.train.Saver(model_vars)\n",
        "                saver.restore(sess, os.path.join(dir_name, (self.subclass_name + \"_%d\") % epoch))\n",
        "                for item in tf.global_variables():\n",
        "                    kv_dict[item.name] = sess.run(item)\n",
        "\n",
        "            # assign to now graph\n",
        "            backup_graph.as_default()\n",
        "            model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.name)\n",
        "            for item in model_vars:\n",
        "                old_name = item.name.replace(self.name, name)\n",
        "                self.sess.run(tf.assign(item, kv_dict[old_name]))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "66BhghHxxBZQ"
      },
      "source": [
        "#@title Replay buffer for deep Q network\n",
        "\"\"\"Replay buffer for deep q network\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"a circular queue based on numpy array, supporting batch put and batch get\"\"\"\n",
        "    def __init__(self, shape, dtype=np.float32):\n",
        "        self.buffer = np.empty(shape=shape, dtype=dtype)\n",
        "        self.head   = 0\n",
        "        self.capacity   = len(self.buffer)\n",
        "\n",
        "    def put(self, data):\n",
        "        \"\"\"put data to\n",
        "        Parameters\n",
        "        ----------\n",
        "        data: numpy array\n",
        "            data to add\n",
        "        \"\"\"\n",
        "        head = self.head\n",
        "        n = len(data)\n",
        "        if head + n <= self.capacity:\n",
        "            self.buffer[head:head+n] = data\n",
        "            self.head = (self.head + n) % self.capacity\n",
        "        else:\n",
        "            split = self.capacity - head\n",
        "            self.buffer[head:] = data[:split]\n",
        "            self.buffer[:n - split] = data[split:]\n",
        "            self.head = split\n",
        "        return n\n",
        "\n",
        "    def get(self, index):\n",
        "        \"\"\"get items\n",
        "        Parameters\n",
        "        ----------\n",
        "        index: int or numpy array\n",
        "            it can be any numpy supported index\n",
        "        \"\"\"\n",
        "        return self.buffer[index]\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"clear replay buffer\"\"\"\n",
        "        self.head = 0"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1wz7aRlBwnKp"
      },
      "source": [
        "#@title Deep Q Network\n",
        "\"\"\"Deep q network\"\"\"\n",
        "\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "class DeepQNetwork(TFBaseModel):\n",
        "    def __init__(self, env, handle, name,\n",
        "                 batch_size=64, learning_rate=1e-4, reward_decay=0.99,\n",
        "                 train_freq=1, target_update=2000, memory_size=2 ** 20, eval_obs=None,\n",
        "                 use_dueling=True, use_double=True, use_conv=True,\n",
        "                 custom_view_space=None, custom_feature_space=None,\n",
        "                 num_gpu=1, infer_batch_size=8192, network_type=0):\n",
        "        \"\"\"init a model\n",
        "        Parameters\n",
        "        ----------\n",
        "        env: Environment\n",
        "            environment\n",
        "        handle: Handle (ctypes.c_int32)\n",
        "            handle of this group, can be got by env.get_handles\n",
        "        name: str\n",
        "            name of this model\n",
        "        learning_rate: float\n",
        "        batch_size: int\n",
        "        reward_decay: float\n",
        "            reward_decay in TD\n",
        "        train_freq: int\n",
        "            mean training times of a sample\n",
        "        target_update: int\n",
        "            target will update every target_update batches\n",
        "        memory_size: int\n",
        "            weight of entropy loss in total loss\n",
        "        eval_obs: numpy array\n",
        "            evaluation set of observation\n",
        "        use_dueling: bool\n",
        "            whether use dueling q network\n",
        "        use_double: bool\n",
        "            whether use double q network\n",
        "        use_conv: bool\n",
        "            use convolution or fully connected layer as state encoder\n",
        "        num_gpu: int\n",
        "            number of gpu\n",
        "        infer_batch_size: int\n",
        "            batch size while inferring actions\n",
        "        custom_feature_space: tuple\n",
        "            customized feature space\n",
        "        custom_view_space: tuple\n",
        "            customized feature space\n",
        "        \"\"\"\n",
        "        TFBaseModel.__init__(self, env, handle, name, \"tfdqn\")\n",
        "        # ======================== set config  ========================\n",
        "        self.env = env\n",
        "        self.handle = handle\n",
        "        self.view_space = custom_view_space or env.get_view_space(handle)\n",
        "        self.feature_space = custom_feature_space or env.get_feature_space(handle)\n",
        "        self.num_actions  = env.get_action_space(handle)[0]\n",
        "\n",
        "        self.batch_size   = batch_size\n",
        "        self.learning_rate= learning_rate\n",
        "        self.train_freq   = train_freq     # train time of every sample (s,a,r,s')\n",
        "        self.target_update= target_update  # target network update frequency\n",
        "        self.eval_obs     = eval_obs\n",
        "        self.infer_batch_size = infer_batch_size  # maximum batch size when infer actions,\n",
        "        # change this to fit your GPU memory if you meet a OOM\n",
        "\n",
        "        self.use_dueling  = use_dueling\n",
        "        self.use_double   = use_double\n",
        "        self.num_gpu      = num_gpu\n",
        "        self.network_type = network_type\n",
        "\n",
        "        self.train_ct = 0\n",
        "\n",
        "        # ======================= build network =======================\n",
        "        # input place holder\n",
        "        self.target = tf.placeholder(tf.float32, [None])\n",
        "        self.weight = tf.placeholder(tf.float32, [None])\n",
        "\n",
        "        self.input_view    = tf.placeholder(tf.float32, (None,) + self.view_space)\n",
        "        self.input_feature = tf.placeholder(tf.float32, (None,) + self.feature_space)\n",
        "        self.action = tf.placeholder(tf.int32, [None])\n",
        "        self.mask   = tf.placeholder(tf.float32, [None])\n",
        "        self.eps = tf.placeholder(tf.float32)  # e-greedy\n",
        "\n",
        "        # build graph\n",
        "        with tf.variable_scope(self.name):\n",
        "            with tf.variable_scope(\"eval_net_scope\"):\n",
        "                self.eval_scope_name   = tf.get_variable_scope().name\n",
        "                self.qvalues = self._create_network(self.input_view, self.input_feature, use_conv)\n",
        "\n",
        "            if self.num_gpu > 1:  # build inference graph for multiple gpus\n",
        "                self._build_multi_gpu_infer(self.num_gpu)\n",
        "\n",
        "            with tf.variable_scope(\"target_net_scope\"):\n",
        "                self.target_scope_name = tf.get_variable_scope().name\n",
        "                self.target_qvalues = self._create_network(self.input_view, self.input_feature, use_conv)\n",
        "\n",
        "        # loss\n",
        "        self.gamma = reward_decay\n",
        "        self.actions_onehot = tf.one_hot(self.action, self.num_actions)\n",
        "        td_error = tf.square(self.target - tf.reduce_sum(tf.multiply(self.actions_onehot, self.qvalues), axis=1))\n",
        "        self.loss = tf.reduce_sum(td_error * self.mask) / tf.reduce_sum(self.mask)\n",
        "\n",
        "        # train op (clip gradient)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "        gradients, variables = zip(*optimizer.compute_gradients(self.loss))\n",
        "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
        "        self.train_op = optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        # output action\n",
        "        def out_action(qvalues):\n",
        "            best_action = tf.argmax(qvalues, axis=1)\n",
        "            best_action = tf.to_int32(best_action)\n",
        "            random_action = tf.random_uniform(tf.shape(best_action), 0, self.num_actions, tf.int32)\n",
        "            should_explore = tf.random_uniform(tf.shape(best_action), 0, 1) < self.eps\n",
        "            return tf.where(should_explore, random_action, best_action)\n",
        "\n",
        "        self.output_action = out_action(self.qvalues)\n",
        "        if self.num_gpu > 1:\n",
        "            self.infer_out_action = [out_action(qvalue) for qvalue in self.infer_qvalues]\n",
        "\n",
        "        # target network update op\n",
        "        self.update_target_op = []\n",
        "        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.target_scope_name)\n",
        "        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, self.eval_scope_name)\n",
        "        for i in range(len(t_params)):\n",
        "            self.update_target_op.append(tf.assign(t_params[i], e_params[i]))\n",
        "\n",
        "        # init tensorflow session\n",
        "        config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
        "        config.gpu_options.allow_growth = True\n",
        "        self.sess = tf.Session(config=config)\n",
        "        self.sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # init replay buffers\n",
        "        self.replay_buf_len = 0\n",
        "        self.memory_size = memory_size\n",
        "        self.replay_buf_view     = ReplayBuffer(shape=(memory_size,) + self.view_space)\n",
        "        self.replay_buf_feature  = ReplayBuffer(shape=(memory_size,) + self.feature_space)\n",
        "        self.replay_buf_action   = ReplayBuffer(shape=(memory_size,), dtype=np.int32)\n",
        "        self.replay_buf_reward   = ReplayBuffer(shape=(memory_size,))\n",
        "        self.replay_buf_terminal = ReplayBuffer(shape=(memory_size,), dtype=np.bool)\n",
        "        self.replay_buf_mask     = ReplayBuffer(shape=(memory_size,))\n",
        "        # if mask[i] == 0, then the item is used for padding, not for training\n",
        "\n",
        "    def _create_network(self, input_view, input_feature, use_conv=True, reuse=None):\n",
        "        \"\"\"define computation graph of network\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_view: tf.tensor\n",
        "        input_feature: tf.tensor\n",
        "            the input tensor\n",
        "        \"\"\"\n",
        "        kernel_num  = [32, 32]\n",
        "        hidden_size = [256]\n",
        "\n",
        "        if use_conv:  # convolution\n",
        "            h_conv1 = tf.layers.conv2d(input_view, filters=kernel_num[0], kernel_size=3,\n",
        "                                       activation=tf.nn.relu, name=\"conv1\", reuse=reuse)\n",
        "            h_conv2 = tf.layers.conv2d(h_conv1, filters=kernel_num[1], kernel_size=3,\n",
        "                                       activation=tf.nn.relu, name=\"conv2\", reuse=reuse)\n",
        "            flatten_view = tf.reshape(h_conv2, [-1, np.prod([v.value for v in h_conv2.shape[1:]])])\n",
        "            h_view = tf.layers.dense(flatten_view, units=hidden_size[0], activation=tf.nn.relu,\n",
        "                                     name=\"dense_view\", reuse=reuse)\n",
        "        else:         # fully connected\n",
        "            flatten_view = tf.reshape(input_view, [-1, np.prod([v.value for v in input_view.shape[1:]])])\n",
        "            h_view = tf.layers.dense(flatten_view, units=hidden_size[0], activation=tf.nn.relu)\n",
        "\n",
        "        h_emb = tf.layers.dense(input_feature,  units=hidden_size[0], activation=tf.nn.relu,\n",
        "                                name=\"dense_emb\", reuse=reuse)\n",
        "\n",
        "        dense = tf.concat([h_view, h_emb], axis=1)\n",
        "\n",
        "        if self.use_dueling:\n",
        "            value = tf.layers.dense(dense, units=1, name=\"value\", reuse=reuse)\n",
        "            advantage = tf.layers.dense(dense, units=self.num_actions, use_bias=False,\n",
        "                                        name=\"advantage\", reuse=reuse)\n",
        "\n",
        "            qvalues = value + advantage - tf.reduce_mean(advantage, axis=1, keep_dims=True)\n",
        "        else:\n",
        "            qvalues = tf.layers.dense(dense, units=self.num_actions, name=\"value\", reuse=reuse)\n",
        "\n",
        "        return qvalues\n",
        "\n",
        "    def infer_action(self, raw_obs, ids, policy='e_greedy', eps=0):\n",
        "        \"\"\"infer action for a batch of agents\n",
        "        Parameters\n",
        "        ----------\n",
        "        raw_obs: tuple(numpy array, numpy array)\n",
        "            raw observation of agents tuple(views, features)\n",
        "        ids: numpy array\n",
        "            ids of agents\n",
        "        policy: str\n",
        "            can be eps-greedy or greedy\n",
        "        eps: float\n",
        "            used when policy is eps-greedy\n",
        "        Returns\n",
        "        -------\n",
        "        acts: numpy array of int32\n",
        "            actions for agents\n",
        "        \"\"\"\n",
        "        view, feature = raw_obs[0], raw_obs[1]\n",
        "\n",
        "        if policy == 'e_greedy':\n",
        "            eps = eps\n",
        "        elif policy == 'greedy':\n",
        "            eps = 0\n",
        "\n",
        "        n = len(view)\n",
        "        batch_size = min(n, self.infer_batch_size)\n",
        "\n",
        "        if self.num_gpu > 1 and n > batch_size:   # infer by multi gpu in parallel\n",
        "            ret = self._infer_multi_gpu(view, feature, ids, eps)\n",
        "        else:                  # infer by splitting big batch in serial\n",
        "            ret = []\n",
        "            for i in range(0, n, batch_size):\n",
        "                beg, end = i, i + batch_size\n",
        "                ret.append(self.sess.run(self.output_action, feed_dict={\n",
        "                    self.input_view: view[beg:end],\n",
        "                    self.input_feature: feature[beg:end],\n",
        "                    self.eps: eps}))\n",
        "            ret = np.concatenate(ret)\n",
        "        return ret\n",
        "\n",
        "    def _calc_target(self, next_view, next_feature, rewards, terminal):\n",
        "        \"\"\"calculate target value\"\"\"\n",
        "        n = len(rewards)\n",
        "        if self.use_double:\n",
        "            t_qvalues, qvalues = self.sess.run([self.target_qvalues, self.qvalues],\n",
        "                                               feed_dict={self.input_view: next_view,\n",
        "                                                          self.input_feature: next_feature})\n",
        "            next_value = t_qvalues[np.arange(n), np.argmax(qvalues, axis=1)]\n",
        "        else:\n",
        "            t_qvalues = self.sess.run(self.target_qvalues, {self.input_view: next_view,\n",
        "                                                            self.input_feature: next_feature})\n",
        "            next_value = np.max(t_qvalues, axis=1)\n",
        "\n",
        "        target = np.where(terminal, rewards, rewards + self.gamma * next_value)\n",
        "\n",
        "        return target\n",
        "\n",
        "    def _add_to_replay_buffer(self, sample_buffer):\n",
        "        \"\"\"add samples in sample_buffer to replay buffer\"\"\"\n",
        "        n = 0\n",
        "        for episode in sample_buffer.episodes():\n",
        "            v, f, a, r = episode.views, episode.features, episode.actions, episode.rewards\n",
        "\n",
        "            m = len(r)\n",
        "\n",
        "            mask = np.ones((m,))\n",
        "            terminal = np.zeros((m,), dtype=np.bool)\n",
        "            if episode.terminal:\n",
        "                terminal[-1] = True\n",
        "            else:\n",
        "                mask[-1] = 0\n",
        "\n",
        "            self.replay_buf_view.put(v)\n",
        "            self.replay_buf_feature.put(f)\n",
        "            self.replay_buf_action.put(a)\n",
        "            self.replay_buf_reward.put(r)\n",
        "            self.replay_buf_terminal.put(terminal)\n",
        "            self.replay_buf_mask.put(mask)\n",
        "\n",
        "            n += m\n",
        "\n",
        "        self.replay_buf_len = min(self.memory_size, self.replay_buf_len + n)\n",
        "        return n\n",
        "\n",
        "    def train(self, sample_buffer, print_every=1000):\n",
        "        \"\"\" add new samples in sample_buffer to replay buffer and train\n",
        "        Parameters\n",
        "        ----------\n",
        "        sample_buffer: buffer.EpisodesBuffer\n",
        "            buffer contains samples\n",
        "        print_every: int\n",
        "            print log every print_every batches\n",
        "        Returns\n",
        "        -------\n",
        "        loss: float\n",
        "            bellman residual loss\n",
        "        value: float\n",
        "            estimated state value\n",
        "        \"\"\"\n",
        "        add_num = self._add_to_replay_buffer(sample_buffer)\n",
        "        batch_size = self.batch_size\n",
        "        total_loss = 0\n",
        "\n",
        "        n_batches = int(self.train_freq * add_num / batch_size)\n",
        "        if n_batches == 0:\n",
        "            return 0, 0\n",
        "\n",
        "        print(\"batch number: %d  add: %d  replay_len: %d/%d\" %\n",
        "              (n_batches, add_num, self.replay_buf_len, self.memory_size))\n",
        "\n",
        "        start_time = time.time()\n",
        "        ct = 0\n",
        "        for i in range(n_batches):\n",
        "            # fetch a batch\n",
        "            index = np.random.choice(self.replay_buf_len - 1, batch_size)\n",
        "\n",
        "            batch_view     = self.replay_buf_view.get(index)\n",
        "            batch_feature  = self.replay_buf_feature.get(index)\n",
        "            batch_action   = self.replay_buf_action.get(index)\n",
        "            batch_reward   = self.replay_buf_reward.get(index)\n",
        "            batch_terminal = self.replay_buf_terminal.get(index)\n",
        "            batch_mask     = self.replay_buf_mask.get(index)\n",
        "\n",
        "            batch_next_view    = self.replay_buf_view.get(index+1)\n",
        "            batch_next_feature = self.replay_buf_feature.get(index+1)\n",
        "\n",
        "            batch_target = self._calc_target(batch_next_view, batch_next_feature,\n",
        "                                             batch_reward, batch_terminal)\n",
        "\n",
        "            ret = self.sess.run([self.train_op, self.loss], feed_dict={\n",
        "                self.input_view:    batch_view,\n",
        "                self.input_feature: batch_feature,\n",
        "                self.action:        batch_action,\n",
        "                self.target:        batch_target,\n",
        "                self.mask:          batch_mask\n",
        "            })\n",
        "            loss = ret[1]\n",
        "            total_loss += loss\n",
        "\n",
        "            if ct % self.target_update == 0:\n",
        "                self.sess.run(self.update_target_op)\n",
        "\n",
        "            if ct % print_every == 0:\n",
        "                print(\"batch %5d,  loss %.6f, eval %.6f\" % (ct, loss, self._eval(batch_target)))\n",
        "            ct += 1\n",
        "            self.train_ct += 1\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        step_average = total_time / max(1.0, (ct / 1000.0))\n",
        "        print(\"batches: %d,  total time: %.2f,  1k average: %.2f\" % (ct, total_time, step_average))\n",
        "\n",
        "        return total_loss / ct if ct != 0 else 0, self._eval(batch_target)\n",
        "\n",
        "    def _eval(self, target):\n",
        "        \"\"\"evaluate estimated q value\"\"\"\n",
        "        if self.eval_obs is None:\n",
        "            return np.mean(target)\n",
        "        else:\n",
        "            return np.mean(self.sess.run([self.qvalues], feed_dict={\n",
        "                self.input_view: self.eval_obs[0],\n",
        "                self.input_feature: self.eval_obs[1]\n",
        "            }))\n",
        "\n",
        "    def clear_buffer(self):\n",
        "        \"\"\"clear replay buffer\"\"\"\n",
        "        self.replay_buf_len = 0\n",
        "        self.replay_buf_view.clear()\n",
        "        self.replay_buf_feature.clear()\n",
        "        self.replay_buf_action.clear()\n",
        "        self.replay_buf_reward.clear()\n",
        "        self.replay_buf_terminal.clear()\n",
        "        self.replay_buf_mask.clear()\n",
        "\n",
        "    def _build_multi_gpu_infer(self, num_gpu):\n",
        "        \"\"\"build inference graph for multi gpus\"\"\"\n",
        "        self.infer_qvalues = []\n",
        "        self.infer_input_view = []\n",
        "        self.infer_input_feature = []\n",
        "        for i in range(num_gpu):\n",
        "            self.infer_input_view.append(tf.placeholder(tf.float32, (None,) + self.view_space))\n",
        "            self.infer_input_feature.append(tf.placeholder(tf.float32, (None,) + self.feature_space))\n",
        "            with tf.variable_scope(\"eval_net_scope\"), tf.device(\"/gpu:%d\" % i):\n",
        "                self.infer_qvalues.append(self._create_network(self.infer_input_view[i],\n",
        "                                                               self.infer_input_feature[i], reuse=True))\n",
        "\n",
        "    def _infer_multi_gpu(self, view, feature, ids, eps):\n",
        "        \"\"\"infer action by multi gpu in parallel \"\"\"\n",
        "        ret = []\n",
        "        beg = 0\n",
        "        while beg < len(view):\n",
        "            feed_dict = {self.eps: eps}\n",
        "            for i in range(self.num_gpu):\n",
        "                end = beg + self.infer_batch_size\n",
        "                feed_dict[self.infer_input_view[i]] = view[beg:end]\n",
        "                feed_dict[self.infer_input_feature[i]] = feature[beg:end]\n",
        "                beg += self.infer_batch_size\n",
        "\n",
        "            ret.extend(self.sess.run(self.infer_out_action, feed_dict=feed_dict))\n",
        "        return np.concatenate(ret)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "_DCftJZ_s2c1",
        "outputId": "313baa16-e447-4ea5-cfb9-4d71f8ee4280"
      },
      "source": [
        "import magent\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    map_size = 100\n",
        "\n",
        "    # init the game \"pursuit\"  (config file are stored in python/magent/builtin/config/)\n",
        "    env = magent.GridWorld(\"pursuit\", map_size=map_size)\n",
        "    env.set_render_dir(\"build/render\")\n",
        "\n",
        "    # get group handles\n",
        "    predator, prey = env.get_handles()\n",
        "\n",
        "    # init env and agents\n",
        "    env.reset()\n",
        "    env.add_walls(method=\"random\", n=map_size * map_size * 0.01)\n",
        "    env.add_agents(predator, method=\"random\", n=map_size * map_size * 0.02)\n",
        "    env.add_agents(prey,     method=\"random\", n=map_size * map_size * 0.02)\n",
        "\n",
        "    # init two models\n",
        "    model1 = DeepQNetwork(env, predator, \"predator\")\n",
        "    model2 = DeepQNetwork(env, prey,     \"prey\")\n",
        "\n",
        "    # load trained model\n",
        "    model1.load(\"data/pursuit_model\")\n",
        "    model2.load(\"data/pursuit_model\")\n",
        "\n",
        "    done = False\n",
        "    step_ct = 0\n",
        "    print(\"nums: %d vs %d\" % (env.get_num(predator), env.get_num(prey)))\n",
        "    while not done:\n",
        "        # take actions for deers\n",
        "        obs_1 = env.get_observation(predator)\n",
        "        ids_1 = env.get_agent_id(predator)\n",
        "        acts_1 = model1.infer_action(obs_1, ids_1)\n",
        "        env.set_action(predator, acts_1)\n",
        "\n",
        "        # take actions for tigers\n",
        "        obs_2  = env.get_observation(prey)\n",
        "        ids_2  = env.get_agent_id(prey)\n",
        "        acts_2 = model2.infer_action(obs_2, ids_1)\n",
        "        env.set_action(prey, acts_2)\n",
        "\n",
        "        # simulate one step\n",
        "        done = env.step()\n",
        "\n",
        "        # render\n",
        "        env.render()\n",
        "\n",
        "        # get reward\n",
        "        reward = [sum(env.get_reward(predator)), sum(env.get_reward(prey))]\n",
        "\n",
        "        # clear dead agents\n",
        "        env.clear_dead()\n",
        "\n",
        "        # print info\n",
        "        if step_ct % 10 == 0:\n",
        "            print(\"step: %d\\t predators' reward: %d\\t preys' reward: %d\" %\n",
        "                    (step_ct, reward[0], reward[1]))\n",
        "\n",
        "        step_ct += 1\n",
        "        if step_ct > 250:\n",
        "            break"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d59a65570a0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# init the game \"pursuit\"  (config file are stored in python/magent/builtin/config/)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pursuit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_render_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# get group handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/magent/gridworld.py\u001b[0m in \u001b[0;36mset_render_dir\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;34m\"\"\" set directory to save render file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         \u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_config_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb\"render_dir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRWo7WAXtMEo"
      },
      "source": [
        "***MAgent paper citation:***\n",
        "\n",
        "@inproceedings{zheng2018magent,\n",
        "  title={MAgent: A many-agent reinforcement learning platform for artificial collective intelligence},\n",
        "  author={Zheng, Lianmin and Yang, Jiacheng and Cai, Han and Zhou, Ming and Zhang, Weinan and Wang, Jun and Yu, Yong},\n",
        "  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},\n",
        "  year={2018}\n",
        "}\n"
      ]
    }
  ]
}